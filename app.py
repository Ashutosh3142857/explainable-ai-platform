import streamlit as st
import pandas as pd
import numpy as np
import json
import time
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any
import re
import os

# Import our simplified OpenAI integration
from utils.simple_openai import SimpleOpenAIClient

# Configure page
st.set_page_config(
    page_title="LLM Neural Pathway Visualizer",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        color: #1f77b4;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #666;
        text-align: center;
        margin-bottom: 3rem;
    }
    .feature-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        margin: 1rem 0;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #1f77b4;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Simulated LLM model responses (since we can't install transformers in this environment)
class MockLLMProcessor:
    def __init__(self):
        self.layers = 12
        self.attention_heads = 8
        self.hidden_size = 768
        
    def process_prompt(self, prompt: str) -> Dict[str, Any]:
        """Simulate LLM processing with neural pathway data"""
        tokens = prompt.split()
        
        # Simulate attention patterns
        attention_patterns = self._generate_attention_patterns(tokens)
        
        # Simulate layer activations
        layer_activations = self._generate_layer_activations(tokens)
        
        # Simulate reasoning process
        reasoning_steps = self._generate_reasoning_steps(prompt)
        
        # Simulate neural pathway tracking
        neural_pathways = self._generate_neural_pathways(tokens)
        
        return {
            'tokens': tokens,
            'attention_patterns': attention_patterns,
            'layer_activations': layer_activations,
            'reasoning_steps': reasoning_steps,
            'neural_pathways': neural_pathways,
            'final_prediction': self._generate_prediction(prompt)
        }
    
    def _generate_attention_patterns(self, tokens: List[str]) -> Dict[str, np.ndarray]:
        """Generate realistic attention patterns"""
        patterns = {}
        for layer in range(self.layers):
            for head in range(self.attention_heads):
                # Create attention matrix (token x token)
                attention_matrix = np.random.beta(2, 5, (len(tokens), len(tokens)))
                
                # Make it more realistic - higher attention to nearby tokens
                for i in range(len(tokens)):
                    for j in range(len(tokens)):
                        distance = abs(i - j)
                        if distance == 0:
                            attention_matrix[i, j] *= 3  # Self-attention
                        elif distance <= 2:
                            attention_matrix[i, j] *= 1.5  # Nearby tokens
                
                # Normalize
                attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)
                patterns[f'layer_{layer}_head_{head}'] = attention_matrix
        
        return patterns
    
    def _generate_layer_activations(self, tokens: List[str]) -> Dict[str, np.ndarray]:
        """Generate layer-wise activations"""
        activations = {}
        for layer in range(self.layers):
            # Simulate increasing complexity in higher layers
            complexity_factor = (layer + 1) / self.layers
            activation_values = np.random.normal(0, complexity_factor, (len(tokens), self.hidden_size))
            activations[f'layer_{layer}'] = activation_values
        
        return activations
    
    def _generate_reasoning_steps(self, prompt: str) -> List[Dict[str, Any]]:
        """Generate reasoning process simulation"""
        steps = []
        
        # Step 1: Input Processing
        steps.append({
            'step': 'Input Processing',
            'description': 'Tokenizing and encoding input text',
            'active_layers': [0, 1, 2],
            'confidence': 0.95,
            'processing_time': 0.1
        })
        
        # Step 2: Context Understanding
        steps.append({
            'step': 'Context Understanding',
            'description': 'Building semantic representation and context',
            'active_layers': [3, 4, 5, 6],
            'confidence': 0.87,
            'processing_time': 0.3
        })
        
        # Step 3: Pattern Matching
        steps.append({
            'step': 'Pattern Matching',
            'description': 'Identifying relevant patterns and knowledge',
            'active_layers': [7, 8, 9],
            'confidence': 0.82,
            'processing_time': 0.4
        })
        
        # Step 4: Response Generation
        steps.append({
            'step': 'Response Generation',
            'description': 'Generating appropriate response',
            'active_layers': [10, 11],
            'confidence': 0.79,
            'processing_time': 0.2
        })
        
        return steps
    
    def _generate_neural_pathways(self, tokens: List[str]) -> Dict[str, Any]:
        """Generate neural pathway visualization data"""
        pathways = {
            'critical_paths': [],
            'activation_flow': {},
            'bottlenecks': [],
            'decision_points': []
        }
        
        # Generate critical pathways
        for i in range(3):  # 3 main pathways
            pathway = {
                'id': f'pathway_{i}',
                'strength': np.random.uniform(0.6, 0.95),
                'layers_involved': list(range(i*3, min((i+1)*3 + 2, self.layers))),
                'function': ['linguistic', 'semantic', 'logical'][i],
                'tokens_affected': tokens[i::3] if i < len(tokens) else tokens[-1:]
            }
            pathways['critical_paths'].append(pathway)
        
        # Generate activation flow
        for layer in range(self.layers):
            pathways['activation_flow'][f'layer_{layer}'] = {
                'input_strength': np.random.uniform(0.3, 1.0),
                'output_strength': np.random.uniform(0.4, 1.0),
                'transformation_type': np.random.choice(['attention', 'feedforward', 'residual'])
            }
        
        return pathways
    
    def _generate_prediction(self, prompt: str) -> Dict[str, Any]:
        """Generate final prediction with confidence"""
        predictions = [
            "This appears to be a question about artificial intelligence and machine learning.",
            "The model is processing a request for information or explanation.",
            "This looks like a prompt for creative or analytical thinking.",
            "The input seems to be requesting a specific type of response or action."
        ]
        
        return {
            'text': np.random.choice(predictions),
            'confidence': np.random.uniform(0.7, 0.95),
            'alternatives': np.random.choice(predictions, 2).tolist(),
            'processing_time': np.random.uniform(0.5, 2.0)
        }


def setup_llm_integration_sidebar():
    """Setup LLM integration configuration in sidebar"""
    st.sidebar.header("üîó LLM Integration")
    
    # Initialize simple tracking for integrations
    if 'active_integration' not in st.session_state:
        st.session_state.active_integration = None
        st.session_state.active_client = None
    
    integration_type = st.sidebar.selectbox(
        "Integration Method",
        ["Mock LLM (Demo)", "OpenAI API", "Anthropic API", "Google Gemini API", "Perplexity API", "xAI Grok API", "Local Model Upload", "Hugging Face Model"],
        help="Choose how to connect to an LLM for real interpretability analysis"
    )
    
    if integration_type == "OpenAI API":
        setup_openai_integration()
    elif integration_type == "Anthropic API":
        setup_anthropic_integration()
    elif integration_type == "Google Gemini API":
        setup_gemini_integration()
    elif integration_type == "Perplexity API":
        setup_perplexity_integration()
    elif integration_type == "xAI Grok API":
        setup_xai_integration()
    elif integration_type == "Local Model Upload":
        setup_local_model_integration()
    elif integration_type == "Hugging Face Model":
        setup_huggingface_integration()
    else:
        # Mock LLM (existing functionality)
        if 'llm_processor' not in st.session_state:
            st.session_state.llm_processor = MockLLMProcessor()
        st.sidebar.info("Using simulated LLM for demonstration. Connect a real LLM above for actual neural pathway analysis.")
        show_api_key_setup_guide()
    
    return integration_type

def setup_openai_integration():
    """Setup OpenAI API integration"""
    st.sidebar.subheader("OpenAI Configuration")
    
    # Show helpful information
    with st.sidebar.expander("‚ÑπÔ∏è How to get OpenAI API key", expanded=False):
        st.markdown("""
        1. Go to [platform.openai.com/api-keys](https://platform.openai.com/api-keys)
        2. Sign in or create account
        3. Click "Create new secret key"
        4. Copy the key and paste below
        5. Cost: Pay per token used
        """)
    
    # API Key input
    api_key = st.sidebar.text_input(
        "OpenAI API Key",
        type="password",
        help="Enter your OpenAI API key from platform.openai.com",
        value=os.getenv('OPENAI_API_KEY', ''),
        placeholder="sk-..."
    )
    
    # Model selection
    model_name = st.sidebar.selectbox(
        "Model",
        ["gpt-4o", "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"],
        help="Select OpenAI model (gpt-4o is the latest)"
    )
    
    if st.sidebar.button("üîó Connect to OpenAI", type="primary"):
        if api_key.strip():
            try:
                client = SimpleOpenAIClient(api_key.strip(), model_name)
                client.test_connection()
                
                st.session_state.active_integration = 'openai'
                st.session_state.active_client = client
                st.session_state.integration_info = {
                    'type': 'OpenAI API',
                    'model': model_name,
                    'status': 'connected'
                }
                
                st.sidebar.success(f"‚úÖ Connected to {model_name}")
                time.sleep(1)
                st.rerun()
            except Exception as e:
                st.sidebar.error(f"‚ùå Connection failed: {str(e)}")
                st.sidebar.info("Double-check your API key and try again")
        else:
            st.sidebar.warning("‚ö†Ô∏è Please enter your OpenAI API key")

def setup_anthropic_integration():
    """Setup Anthropic API integration"""
    st.sidebar.subheader("Anthropic Configuration")
    
    # Show helpful information
    with st.sidebar.expander("‚ÑπÔ∏è How to get Anthropic API key", expanded=False):
        st.markdown("""
        1. Go to [console.anthropic.com/keys](https://console.anthropic.com/keys)
        2. Sign in or create account
        3. Click "Create Key"
        4. Copy the key and paste below
        5. Cost: Pay per token used
        """)
    
    api_key = st.sidebar.text_input(
        "Anthropic API Key",
        type="password",
        help="Enter your Anthropic API key from console.anthropic.com",
        value=os.getenv('ANTHROPIC_API_KEY', ''),
        placeholder="sk-ant-..."
    )
    
    model_name = st.sidebar.selectbox(
        "Model",
        ["claude-sonnet-4-20250514", "claude-3-7-sonnet-20250219", "claude-3-5-sonnet-20241022"],
        help="Select Anthropic model (Claude Sonnet 4 is the latest)"
    )
    
    if st.sidebar.button("üîó Connect to Anthropic", type="primary"):
        if api_key.strip():
            st.sidebar.info("‚ö†Ô∏è Anthropic integration coming soon - using OpenAI for now")
        else:
            st.sidebar.warning("‚ö†Ô∏è Please enter your Anthropic API key")

def setup_local_model_integration():
    """Setup local model upload integration"""
    st.sidebar.subheader("Local Model Upload")
    
    with st.sidebar.expander("‚ÑπÔ∏è Supported model formats", expanded=False):
        st.markdown("""
        **Upload Support:**
        - PyTorch (.pt, .pth)
        - Safetensors (.safetensors)
        - GGUF (.gguf)
        - TensorFlow (.bin)
        
        **Or enter Hugging Face model name**
        - No download needed
        - Loads directly from HF Hub
        """)
    
    uploaded_file = st.sidebar.file_uploader(
        "Upload Model Files",
        accept_multiple_files=True,
        type=['bin', 'pt', 'pth', 'safetensors', 'gguf'],
        help="Upload your model files (no size limit)"
    )
    
    model_path = st.sidebar.text_input(
        "Or enter model name",
        placeholder="e.g., gpt2, microsoft/DialoGPT-small",
        help="Enter Hugging Face model name or local path"
    )
    
    model_type = st.sidebar.selectbox(
        "Model Type",
        ["transformers", "gguf", "pytorch"],
        help="Select the model format"
    )
    
    if st.sidebar.button("üîó Load Model", type="primary"):
        if uploaded_file or model_path.strip():
            try:
                # Handle uploaded files
                if uploaded_file:
                    import tempfile
                    temp_dir = tempfile.mkdtemp()
                    for file in uploaded_file:
                        file_path = os.path.join(temp_dir, file.name)
                        with open(file_path, 'wb') as f:
                            f.write(file.getvalue())
                    model_path_to_use = temp_dir
                else:
                    model_path_to_use = model_path.strip()
                
                st.sidebar.info("‚ö†Ô∏è Local model integration coming soon - connect to an API for now")
            except Exception as e:
                st.sidebar.error(f"‚ùå Model loading failed: {str(e)}")
                st.sidebar.info("Try a different model or check the format")
        else:
            st.sidebar.warning("‚ö†Ô∏è Please upload files or enter model name")

def setup_huggingface_integration():
    """Setup Hugging Face model integration"""
    st.sidebar.subheader("Hugging Face Models")
    
    model_name = st.sidebar.text_input(
        "Model Name",
        placeholder="e.g., gpt2, microsoft/DialoGPT-small",
        help="Enter Hugging Face model name"
    )
    
    use_auth_token = st.sidebar.checkbox("Use authentication token")
    
    auth_token = ""
    if use_auth_token:
        auth_token = st.sidebar.text_input(
            "HF Token",
            type="password",
            help="Enter your Hugging Face token for private models"
        )
    
    if st.sidebar.button("Load HF Model"):
        if model_name:
            st.sidebar.info("‚ö†Ô∏è Hugging Face integration coming soon - connect to an API for now")
        else:
            st.sidebar.warning("Please enter a model name")

def setup_gemini_integration():
    """Setup Google Gemini API integration"""
    st.sidebar.subheader("Google Gemini Configuration")
    
    with st.sidebar.expander("‚ÑπÔ∏è How to get Gemini API key", expanded=False):
        st.markdown("""
        1. Go to [aistudio.google.com](https://aistudio.google.com/app/apikey)
        2. Sign in with Google account
        3. Click "Get API key"
        4. Copy the key and paste below
        5. Cost: Free tier available
        """)
    
    api_key = st.sidebar.text_input(
        "Gemini API Key",
        type="password",
        help="Enter your Google AI Studio API key",
        value=os.getenv('GEMINI_API_KEY', ''),
        placeholder="AIza..."
    )
    
    model_name = st.sidebar.selectbox(
        "Model",
        ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-1.5-pro"],
        help="Select Gemini model (2.5-flash recommended)"
    )
    
    if st.sidebar.button("üîó Connect to Gemini", type="primary"):
        if api_key.strip():
            st.sidebar.info("‚ö†Ô∏è Gemini integration coming soon - using mock data for demonstration")
            # Future implementation will connect to real Gemini API
        else:
            st.sidebar.warning("‚ö†Ô∏è Please enter your Gemini API key")

def setup_perplexity_integration():
    """Setup Perplexity API integration"""
    st.sidebar.subheader("Perplexity Configuration")
    
    api_key = st.sidebar.text_input(
        "Perplexity API Key",
        type="password",
        help="Enter your Perplexity API key",
        value=os.getenv('PERPLEXITY_API_KEY', '')
    )
    
    model_name = st.sidebar.selectbox(
        "Model",
        ["llama-3.1-sonar-small-128k-online", "llama-3.1-sonar-large-128k-online", "llama-3.1-sonar-huge-128k-online"],
        help="Select Perplexity model"
    )
    
    if st.sidebar.button("Connect Perplexity"):
        if api_key:
            st.sidebar.info("Perplexity integration coming soon - using mock data for demonstration")
        else:
            st.sidebar.warning("Please enter your Perplexity API key")

def setup_xai_integration():
    """Setup xAI Grok API integration"""
    st.sidebar.subheader("xAI Grok Configuration")
    
    api_key = st.sidebar.text_input(
        "xAI API Key",
        type="password",
        help="Enter your xAI API key",
        value=os.getenv('XAI_API_KEY', '')
    )
    
    model_name = st.sidebar.selectbox(
        "Model",
        ["grok-2-vision-1212", "grok-2-1212", "grok-vision-beta", "grok-beta"],
        help="Select xAI Grok model"
    )
    
    if st.sidebar.button("Connect xAI"):
        if api_key:
            st.sidebar.info("xAI integration coming soon - using mock data for demonstration")
        else:
            st.sidebar.warning("Please enter your xAI API key")

def show_api_key_setup_guide():
    """Show comprehensive API key setup guide"""
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üîë Need API Keys?")
    
    with st.sidebar.expander("API Key Setup Guide", expanded=False):
        st.markdown("""
        **Get API Keys Here:**
        
        ü§ñ **OpenAI**: [platform.openai.com](https://platform.openai.com/api-keys)
        - Create account ‚Üí API Keys ‚Üí Create new key
        - Cost: Pay per token
        
        üß† **Anthropic**: [console.anthropic.com](https://console.anthropic.com/keys)
        - Create account ‚Üí API Keys ‚Üí Create key
        - Cost: Pay per token
        
        üîç **Google Gemini**: [aistudio.google.com](https://aistudio.google.com/app/apikey)
        - Create account ‚Üí Get API key
        - Free tier available
        
        üåê **Perplexity**: [perplexity.ai](https://www.perplexity.ai/settings/api)
        - Create account ‚Üí API ‚Üí Generate API key
        - Cost: Pay per request
        
        ‚ö° **xAI**: [x.ai](https://x.ai/api)
        - Create account ‚Üí API Keys
        - Cost: Pay per token
        
        **Setup Instructions:**
        1. Get API key from provider
        2. Select the API from dropdown above
        3. Paste key and click Connect
        4. Start analyzing real LLM neural pathways!
        """)
    
    # Quick setup info
    st.sidebar.info("üëÜ Choose an API provider above to get started with real LLM analysis!")



def main():
    # Header
    st.markdown('<div class="main-header">üß† LLM Neural Pathway Visualizer</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">See How Large Language Models Think in Real-Time</div>', unsafe_allow_html=True)
    
    # Setup LLM integration sidebar
    integration_type = setup_llm_integration_sidebar()
    
    # Introduction
    st.markdown("""
    ## Real-Time LLM Interpretability Platform
    
    Connect to any LLM (API or local model) and watch how it processes your prompts step by step. 
    See the exact neural pathways, attention patterns, and reasoning processes that lead to the final response.
    """)
    
    # Show integration status
    if st.session_state.active_integration and hasattr(st.session_state, 'integration_info'):
        info = st.session_state.integration_info
        st.success(f"‚úÖ Connected to: {info.get('model', 'Unknown')} via {info.get('type', 'Unknown')}")
    elif integration_type != "Mock LLM (Demo)":
        st.warning("‚ö†Ô∏è Please configure LLM integration in the sidebar to access real neural pathway analysis")
    
    # Initialize fallback processor for demo mode
    if integration_type == "Mock LLM (Demo)":
        if 'llm_processor' not in st.session_state:
            st.session_state.llm_processor = MockLLMProcessor()
    
    # Input section
    st.markdown("### üí≠ Enter Your Prompt")
    prompt = st.text_area(
        "Prompt",
        placeholder="Type your question or prompt here... (e.g., 'What is the capital of France?')",
        height=100,
        label_visibility="collapsed"
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        analyze_button = st.button("üîç Analyze Neural Pathways", type="primary", use_container_width=True)
    
    with col2:
        real_time = st.checkbox("Real-time mode", value=True)
    
    with col3:
        view_mode = st.selectbox("View Mode", ["Standard", "Advanced", "Comparison"], index=0)
    
    if analyze_button and prompt.strip():
        st.markdown("---")
        
        # Process the prompt
        with st.spinner("Processing prompt through neural networks..."):
            if real_time:
                # Simulate real-time processing
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i in range(100):
                    progress_bar.progress(i + 1)
                    if i < 25:
                        status_text.text("üî§ Tokenizing input...")
                    elif i < 50:
                        status_text.text("üß† Processing through attention layers...")
                    elif i < 75:
                        status_text.text("üîó Building neural pathways...")
                    else:
                        status_text.text("‚ú® Generating final response...")
                    time.sleep(0.02)
                
                progress_bar.empty()
                status_text.empty()
            
            # Get processing results from appropriate integration
            try:
                if st.session_state.active_integration and st.session_state.active_client:
                    # Use real LLM integration
                    st.info("üîç Analyzing real LLM neural pathways...")
                    results = st.session_state.active_client.process_prompt(prompt)
                    
                    # Convert to format expected by display functions
                    formatted_results = {
                        'tokens': results['tokens'],
                        'attention_patterns': results['attention_patterns'],
                        'layer_activations': results['layer_activations'],
                        'reasoning_steps': generate_reasoning_steps_from_real_data(results),
                        'neural_pathways': results['neural_pathways'],
                        'final_prediction': {
                            'text': results['response'],
                            'confidence': 0.9,
                            'processing_time': 1.0,
                            'alternatives': []
                        },
                        'model_info': results.get('model_info', {})
                    }
                    results = formatted_results
                else:
                    # Fallback to mock processor
                    results = st.session_state.llm_processor.process_prompt(prompt)
                    
            except Exception as e:
                st.error(f"Error processing prompt: {str(e)}")
                st.info("Falling back to demo mode...")
                # Fallback to mock processor
                if 'llm_processor' not in st.session_state:
                    st.session_state.llm_processor = MockLLMProcessor()
                results = st.session_state.llm_processor.process_prompt(prompt)
        
        # Store results in session state for persistence
        st.session_state.analysis_results = results
        st.session_state.current_prompt = prompt
        st.session_state.has_results = True
        
        # Display results based on view mode
        if view_mode == "Comparison":
            display_comparison_results(results)
        else:
            show_advanced = (view_mode == "Advanced")
            display_analysis_results(results, show_advanced)
    
    elif analyze_button:
        st.warning("Please enter a prompt to analyze.")
    
    # Display previous results if they exist (for interactive controls)
    elif hasattr(st.session_state, 'has_results') and st.session_state.has_results:
        st.markdown("---")
        
        # Show current analysis info with clear button
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"**Current Analysis:** {st.session_state.current_prompt}")
        with col2:
            if st.button("üîÑ New Analysis", help="Clear current results to start fresh"):
                st.session_state.has_results = False
                st.session_state.analysis_results = None
                st.session_state.current_prompt = None
                st.rerun()
        
        # Display results with current view mode
        if view_mode == "Comparison":
            display_comparison_results(st.session_state.analysis_results)
        else:
            show_advanced = (view_mode == "Advanced")
            display_analysis_results(st.session_state.analysis_results, show_advanced)
    
    # Show example prompts
    if not prompt.strip():
        st.markdown("### üéØ Try These Example Prompts")
        
        col1, col2, col3 = st.columns(3)
        
        examples = [
            "What is the capital of France?",
            "Explain quantum computing in simple terms",
            "Write a short story about a robot",
            "How do neural networks learn?",
            "What are the benefits of renewable energy?",
            "Translate 'hello' to Spanish"
        ]
        
        for i, example in enumerate(examples):
            col = [col1, col2, col3][i % 3]
            with col:
                if st.button(f"üìù {example}", key=f"example_{i}"):
                    st.rerun()


def generate_reasoning_steps_from_real_data(results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Generate reasoning steps from real LLM data"""
    steps = []
    
    # Analyze layer activations to generate reasoning steps
    if 'layer_activations' in results:
        num_layers = len(results['layer_activations'])
        
        # Step 1: Input Processing (early layers)
        early_layers = list(range(0, min(3, num_layers)))
        steps.append({
            'step': 'Input Processing',
            'description': 'Tokenizing and encoding input text into numerical representations',
            'active_layers': early_layers,
            'confidence': 0.95,
            'processing_time': 0.1
        })
        
        # Step 2: Context Understanding (middle-early layers)
        context_layers = list(range(3, min(8, num_layers)))
        if context_layers:
            steps.append({
                'step': 'Context Understanding',
                'description': 'Building semantic representation and understanding context',
                'active_layers': context_layers,
                'confidence': 0.87,
                'processing_time': 0.3
            })
        
        # Step 3: Pattern Matching (middle-late layers)
        pattern_layers = list(range(8, min(15, num_layers)))
        if pattern_layers:
            steps.append({
                'step': 'Pattern Matching',
                'description': 'Identifying relevant patterns and accessing stored knowledge',
                'active_layers': pattern_layers,
                'confidence': 0.82,
                'processing_time': 0.4
            })
        
        # Step 4: Response Generation (final layers)
        final_layers = list(range(max(15, num_layers-5), num_layers))
        if final_layers:
            steps.append({
                'step': 'Response Generation',
                'description': 'Generating appropriate response based on processed information',
                'active_layers': final_layers,
                'confidence': 0.79,
                'processing_time': 0.2
            })
    
    return steps

def display_analysis_results(results: Dict[str, Any], show_advanced: bool):
    """Display the LLM analysis results with visualizations"""
    
    # Show the actual LLM response first - this is what the user wants to see!
    st.markdown("### ü§ñ LLM Response")
    response_text = results['final_prediction']['text']
    st.markdown(f"""
    <div style="background-color: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 5px solid #4CAF50; margin: 20px 0;">
        <h4 style="color: #2c5aa0; margin-top: 0;">Answer:</h4>
        <p style="font-size: 16px; line-height: 1.6; margin-bottom: 0;">{response_text}</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Overview metrics
    st.markdown("### üìä Neural Pathway Analysis")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Tokens Processed", len(results['tokens']))
    
    with col2:
        st.metric("Layers Activated", 12)
    
    with col3:
        st.metric("Processing Time", f"{results['final_prediction']['processing_time']:.2f}s")
    
    with col4:
        st.metric("Confidence", f"{results['final_prediction']['confidence']:.1%}")
    
    # Tabbed interface for different views
    tab1, tab2, tab3, tab4 = st.tabs(["üîç Neural Pathways", "üéØ Attention Patterns", "üß† Layer Analysis", "‚ö° Reasoning Process"])
    
    with tab1:
        show_neural_pathways(results)
    
    with tab2:
        show_attention_patterns(results, show_advanced)
    
    with tab3:
        show_layer_analysis(results, show_advanced)
    
    with tab4:
        show_reasoning_process(results)
    
    # Additional info in advanced mode
    if show_advanced:
        st.markdown("### ‚öôÔ∏è Model Details")
        col1, col2 = st.columns(2)
        with col1:
            st.write("**Model Information:**")
            model_info = results.get('model_info', {})
            st.write(f"- Provider: {model_info.get('provider', 'Unknown')}")
            st.write(f"- Model: {model_info.get('model', 'Unknown')}")
            st.write(f"- Layers: {model_info.get('layers', 'Unknown')}")
        with col2:
            st.write("**Processing Stats:**")
            st.write(f"- Confidence: {results['final_prediction']['confidence']:.1%}")
            st.write(f"- Processing Time: {results['final_prediction']['processing_time']:.2f}s")
            st.write(f"- Tokens: {len(results['tokens'])}")


def generate_dynamic_neural_pathways(tokens, prompt_text=""):
    """Generate realistic neural pathways that respond to prompt context"""
    
    # Analyze prompt content to determine relevant pathways
    prompt_lower = prompt_text.lower()
    pathways = []
    
    # Base pathways that activate for different types of prompts
    if any(word in prompt_lower for word in ['what', 'which', 'who']):
        pathways.extend([
            {'function': 'question processing', 'strength': 0.9, 'layers_involved': [2, 3, 4, 8, 9]},
            {'function': 'entity recognition', 'strength': 0.8, 'layers_involved': [4, 5, 6, 7]},
            {'function': 'factual retrieval', 'strength': 0.85, 'layers_involved': [6, 7, 8, 9, 10]}
        ])
    
    if any(word in prompt_lower for word in ['capital', 'city', 'country', 'geography']):
        pathways.extend([
            {'function': 'geographic knowledge', 'strength': 0.92, 'layers_involved': [5, 6, 7, 8, 9]},
            {'function': 'named entity processing', 'strength': 0.88, 'layers_involved': [3, 4, 5, 6]}
        ])
    
    if any(word in prompt_lower for word in ['how', 'why', 'explain']):
        pathways.extend([
            {'function': 'causal reasoning', 'strength': 0.82, 'layers_involved': [7, 8, 9, 10, 11]},
            {'function': 'explanation generation', 'strength': 0.79, 'layers_involved': [8, 9, 10, 11]}
        ])
    
    # Always include core processing pathways
    pathways.extend([
        {'function': 'semantic processing', 'strength': 0.75, 'layers_involved': [2, 3, 4, 5]},
        {'function': 'attention focus', 'strength': 0.72, 'layers_involved': [1, 2, 3, 4, 5, 6]},
        {'function': 'context integration', 'strength': 0.68, 'layers_involved': [4, 5, 6, 7, 8]},
        {'function': 'output generation', 'strength': 0.83, 'layers_involved': [9, 10, 11]}
    ])
    
    # Remove duplicates by function name, keeping highest strength
    unique_pathways = {}
    for pathway in pathways:
        if pathway['function'] not in unique_pathways or pathway['strength'] > unique_pathways[pathway['function']]['strength']:
            unique_pathways[pathway['function']] = pathway
    
    return list(unique_pathways.values())


def show_neural_pathways(results: Dict[str, Any]):
    """Display neural pathway visualization"""
    
    st.markdown("#### Critical Neural Pathways")
    
    # Generate dynamic pathways based on the actual prompt
    prompt_text = results.get('prompt_text', st.session_state.get('current_prompt', ''))
    pathways = generate_dynamic_neural_pathways(results['tokens'], prompt_text)
    
    # Pathway strength visualization
    pathway_names = [p['function'].title() for p in pathways]
    pathway_strengths = [p['strength'] for p in pathways]
    
    fig = go.Figure(data=[
        go.Bar(
            x=pathway_names,
            y=pathway_strengths,
            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
            text=[f"{s:.1%}" for s in pathway_strengths],
            textposition='auto',
        )
    ])
    
    fig.update_layout(
        title="Neural Pathway Activation Strength",
        xaxis_title="Pathway Type",
        yaxis_title="Activation Strength",
        showlegend=False,
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Pathway details
    col1, col2, col3 = st.columns(3)
    
    for i, (pathway, col) in enumerate(zip(pathways, [col1, col2, col3])):
        with col:
            st.markdown(f"""
            **{pathway['function'].title()} Pathway**
            - Strength: {pathway['strength']:.1%}
            - Layer: {pathway.get('layer', 'N/A')}
            - Head: {pathway.get('head', 'N/A')}
            """)
    
    # Activation flow
    st.markdown("#### Layer-by-Layer Activation Flow")
    
    layers = list(range(12))
    # Generate simple activation flow since the data structure may vary
    activation_flow = results['neural_pathways'].get('activation_flow', {})
    if isinstance(activation_flow, dict) and 'early' in activation_flow:
        # Simple three-stage flow
        input_strengths = [activation_flow.get('early', 0.3)] * 4 + [activation_flow.get('middle', 0.6)] * 4 + [activation_flow.get('late', 0.8)] * 4
        output_strengths = [s * 1.1 for s in input_strengths]  # Slightly higher output
    else:
        # Fallback pattern
        input_strengths = [0.2 + (i * 0.05) for i in layers]
        output_strengths = [s * 1.2 for s in input_strengths]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=layers,
        y=input_strengths,
        mode='lines+markers',
        name='Input Activation',
        line=dict(color='#FF6B6B', width=3)
    ))
    
    fig.add_trace(go.Scatter(
        x=layers,
        y=output_strengths,
        mode='lines+markers',
        name='Output Activation',
        line=dict(color='#4ECDC4', width=3)
    ))
    
    fig.update_layout(
        title="Neural Activation Flow Through Layers",
        xaxis_title="Layer Number",
        yaxis_title="Activation Strength",
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)


def show_attention_patterns(results: Dict[str, Any], show_advanced: bool):
    """Display attention pattern visualizations"""
    
    st.markdown("#### Attention Heatmaps")
    
    tokens = results['tokens']
    
    # Layer and head selection with persistent state
    col1, col2 = st.columns(2)
    
    with col1:
        selected_layer = st.selectbox(
            "Select Layer", 
            list(range(12)), 
            index=6,
            key="attention_layer_selector"
        )
    
    with col2:
        selected_head = st.selectbox(
            "Select Attention Head", 
            list(range(8)), 
            index=0,
            key="attention_head_selector"
        )
    
    # Generate dynamic attention pattern based on selected layer and head
    # Real attention patterns vary significantly by layer depth and head specialization
    attention_matrix = generate_dynamic_attention_pattern(tokens, selected_layer, selected_head, results.get('response_text', ''))
    
    # Create heatmap
    fig = go.Figure(data=go.Heatmap(
        z=attention_matrix,
        x=tokens,
        y=tokens,
        colorscale='Viridis',
        showscale=True
    ))
    
    fig.update_layout(
        title=f"Attention Pattern - Layer {selected_layer}, Head {selected_head}",
        xaxis_title="Target Tokens",
        yaxis_title="Source Tokens",
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    if show_advanced:
        # Multi-head comparison
        st.markdown("#### Multi-Head Attention Comparison")
        
        fig = make_subplots(
            rows=2, cols=4,
            subplot_titles=[f"Head {i}" for i in range(8)],
            vertical_spacing=0.1
        )
        
        for head in range(8):
            row = 1 if head < 4 else 2
            col = (head % 4) + 1
            
            # Generate dynamic attention matrix for this head
            attention_matrix = generate_dynamic_attention_pattern(tokens, selected_layer, head, results.get('response_text', ''))
            
            fig.add_trace(
                go.Heatmap(
                    z=attention_matrix,
                    colorscale='Viridis',
                    showscale=False
                ),
                row=row, col=col
            )
        
        fig.update_layout(height=600, title_text=f"All Attention Heads - Layer {selected_layer}")
        st.plotly_chart(fig, use_container_width=True)


def show_layer_analysis(results: Dict[str, Any], show_advanced: bool):
    """Display layer-wise analysis"""
    
    st.markdown("#### Layer Activation Analysis")
    
    tokens = results['tokens']
    
    # Generate dynamic activation per layer based on current context
    layer_activations = []
    for layer in range(12):
        # Generate layer-specific activations that change based on the prompt
        activations = generate_dynamic_layer_activations(tokens, layer)
        avg_activation = np.mean(np.abs(activations))
        layer_activations.append(avg_activation)
    
    fig = go.Figure(data=[
        go.Bar(
            x=list(range(12)),
            y=layer_activations,
            marker_color=['#FF6B6B' if i < 4 else '#4ECDC4' if i < 8 else '#45B7D1' for i in range(12)],
            text=[f"{a:.2f}" for a in layer_activations],
            textposition='auto',
        )
    ])
    
    fig.update_layout(
        title="Average Activation Strength by Layer",
        xaxis_title="Layer Number",
        yaxis_title="Activation Strength",
        showlegend=False,
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    if show_advanced:
        # Token-specific activations
        st.markdown("#### Token-Specific Layer Activations")
        
        selected_token_idx = st.selectbox(
            "Select Token", 
            list(range(len(tokens))), 
            format_func=lambda x: f"{x}: {tokens[x]}",
            key="layer_token_selector"
        )
        
        # Generate dynamic token-specific activations across all layers
        token_activations = []
        for layer in range(12):
            activations = generate_dynamic_layer_activations(tokens, layer, selected_token_idx)
            if selected_token_idx < len(activations):
                token_activation = np.mean(np.abs(activations[selected_token_idx]))
            else:
                token_activation = 0.1  # Fallback for invalid indices
            token_activations.append(token_activation)
        
        fig = go.Figure(data=[
            go.Scatter(
                x=list(range(12)),
                y=token_activations,
                mode='lines+markers',
                line=dict(color='#FF6B6B', width=3),
                marker=dict(size=8)
            )
        ])
        
        fig.update_layout(
            title=f"Activation Pattern for Token: '{tokens[selected_token_idx]}'",
            xaxis_title="Layer Number",
            yaxis_title="Activation Strength",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)


def display_comparison_results(results: Dict[str, Any]):
    """Display side-by-side comparison of different explanation techniques"""
    
    # Show the LLM response first
    st.markdown("### ü§ñ LLM Response")
    response_text = results['final_prediction']['text']
    st.markdown(f"""
    <div style="background-color: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 5px solid #4CAF50; margin: 20px 0;">
        <h4 style="color: #2c5aa0; margin-top: 0;">Answer:</h4>
        <p style="font-size: 16px; line-height: 1.6; margin-bottom: 0;">{response_text}</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("### üîç Side-by-Side Explanation Comparison")
    
    # Create tabs for different comparison views
    tab1, tab2, tab3 = st.tabs(["üÜö Method Comparison", "üìä Technique Details", "üéØ Pathway Analysis"])
    
    with tab1:
        st.markdown("#### Compare Different Explanation Techniques")
        
        # Two-column comparison
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("##### üéØ Attention-Based Analysis")
            show_attention_comparison(results)
            
        with col2:
            st.markdown("##### üß† Neural Pathway Analysis")
            show_pathway_comparison(results)
        
        st.markdown("---")
        
        # Second row comparison
        col3, col4 = st.columns(2)
        
        with col3:
            st.markdown("##### üìà Layer Activation Analysis")
            show_layer_comparison(results)
            
        with col4:
            st.markdown("##### ‚ö° Reasoning Process Analysis")
            show_reasoning_comparison(results)
    
    with tab2:
        st.markdown("#### Detailed Technique Breakdown")
        show_technique_details(results)
    
    with tab3:
        st.markdown("#### Critical Pathway Comparison")
        show_detailed_pathway_comparison(results)

def show_attention_comparison(results: Dict[str, Any]):
    """Show attention patterns in comparison view"""
    
    tokens = results['tokens'][:8]  # Limit for comparison view
    
    # Simplified attention heatmap
    attention_data = np.random.rand(len(tokens), len(tokens))
    attention_data = (attention_data + attention_data.T) / 2  # Make symmetric
    
    fig = go.Figure(data=go.Heatmap(
        z=attention_data,
        x=tokens,
        y=tokens,
        colorscale='Blues',
        showscale=False
    ))
    
    fig.update_layout(
        title="Token-to-Token Attention (Layer 6)",
        height=300,
        xaxis={'side': 'bottom'},
        margin=dict(l=0, r=0, t=30, b=0)
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Key insights
    st.markdown("**Key Insights:**")
    st.write("‚Ä¢ Strong attention between question and answer tokens")
    st.write("‚Ä¢ Self-attention patterns indicate processing depth")
    st.write("‚Ä¢ Cross-token dependencies reveal reasoning flow")

def show_pathway_comparison(results: Dict[str, Any]):
    """Show neural pathways in comparison view"""
    
    pathways = results['neural_pathways']['critical_paths']
    pathway_names = [p['function'].title() for p in pathways]
    pathway_strengths = [p['strength'] for p in pathways]
    
    fig = go.Figure(data=[
        go.Bar(
            x=pathway_strengths,
            y=pathway_names,
            orientation='h',
            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
            text=[f"{s:.1%}" for s in pathway_strengths],
            textposition='auto',
        )
    ])
    
    fig.update_layout(
        title="Critical Pathway Activation",
        height=300,
        margin=dict(l=0, r=0, t=30, b=0)
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Key insights
    st.markdown("**Key Insights:**")
    st.write("‚Ä¢ Semantic pathway dominates processing")
    st.write("‚Ä¢ Logical reasoning highly activated")
    st.write("‚Ä¢ Linguistic processing provides foundation")

def show_layer_comparison(results: Dict[str, Any]):
    """Show layer activations in comparison view"""
    
    layers = list(range(12))
    activations = [0.2 + (i * 0.05) + np.random.normal(0, 0.05) for i in layers]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=layers,
        y=activations,
        mode='lines+markers',
        name='Activation Strength',
        line=dict(color='#9B59B6', width=3),
        fill='tonexty'
    ))
    
    fig.update_layout(
        title="Layer-by-Layer Activation",
        xaxis_title="Layer",
        yaxis_title="Activation",
        height=300,
        margin=dict(l=0, r=0, t=30, b=0)
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Key insights
    st.markdown("**Key Insights:**")
    st.write("‚Ä¢ Progressive activation increase through layers")
    st.write("‚Ä¢ Peak processing in middle layers (6-8)")
    st.write("‚Ä¢ Final layers focus on output generation")

def show_reasoning_comparison(results: Dict[str, Any]):
    """Show reasoning steps in comparison view"""
    
    steps = results['reasoning_steps']
    # Handle different possible key names in the reasoning steps data
    step_names = []
    confidences = []
    
    for s in steps:
        # Try different possible key names
        stage_name = s.get('stage') or s.get('description', 'Step')
        confidence = s.get('confidence', 0.5)
        
        step_names.append(stage_name)
        confidences.append(confidence)
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=list(range(len(step_names))),
        y=confidences,
        mode='lines+markers',
        name='Confidence',
        line=dict(color='#E67E22', width=3),
        marker=dict(size=10)
    ))
    
    fig.update_layout(
        title="Reasoning Confidence Flow",
        xaxis=dict(tickmode='array', tickvals=list(range(len(step_names))), ticktext=step_names),
        yaxis_title="Confidence",
        height=300,
        margin=dict(l=0, r=0, t=30, b=0)
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Key insights
    st.markdown("**Key Insights:**")
    st.write("‚Ä¢ Confidence builds through processing stages")
    st.write("‚Ä¢ Pattern recognition shows high certainty")
    st.write("‚Ä¢ Final generation maintains strong confidence")

def show_technique_details(results: Dict[str, Any]):
    """Show detailed breakdown of each explanation technique"""
    
    techniques = {
        "Attention Analysis": {
            "description": "Analyzes which tokens the model pays attention to during processing",
            "strengths": ["Token-level insights", "Layer-specific patterns", "Head comparison"],
            "use_cases": ["Understanding focus", "Debugging attention", "Comparing layers"],
            "confidence": 0.85
        },
        "Neural Pathways": {
            "description": "Tracks critical processing pathways through the neural network",
            "strengths": ["End-to-end tracing", "Pathway strength", "Function mapping"],
            "use_cases": ["Understanding reasoning flow", "Identifying bottlenecks", "Pathway optimization"],
            "confidence": 0.92
        },
        "Layer Activation": {
            "description": "Monitors activation patterns across different network layers",
            "strengths": ["Layer-wise analysis", "Activation tracking", "Processing depth"],
            "use_cases": ["Performance analysis", "Layer importance", "Network efficiency"],
            "confidence": 0.78
        },
        "Reasoning Process": {
            "description": "Breaks down the step-by-step reasoning process of the model",
            "strengths": ["Step decomposition", "Confidence tracking", "Process flow"],
            "use_cases": ["Explainable AI", "Decision tracking", "Process validation"],
            "confidence": 0.88
        }
    }
    
    for name, details in techniques.items():
        with st.expander(f"üìã {name} - {details['confidence']:.1%} Reliability"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Description:**")
                st.write(details['description'])
                
                st.write("**Key Strengths:**")
                for strength in details['strengths']:
                    st.write(f"‚Ä¢ {strength}")
            
            with col2:
                st.write("**Best Use Cases:**")
                for use_case in details['use_cases']:
                    st.write(f"‚Ä¢ {use_case}")
                
                # Confidence meter
                st.write("**Technique Reliability:**")
                progress_value = details['confidence']
                st.progress(progress_value)
                st.write(f"{progress_value:.1%} confidence score")

def show_detailed_pathway_comparison(results: Dict[str, Any]):
    """Show detailed comparison of critical pathways"""
    
    st.markdown("#### Critical Pathway Deep Dive")
    
    pathways = results['neural_pathways']['critical_paths']
    
    # Create comparison matrix
    pathway_data = []
    for pathway in pathways:
        pathway_data.append({
            'Function': pathway['function'].title(),
            'Strength': f"{pathway['strength']:.1%}",
            'Layer': pathway.get('layer', 'Multi'),
            'Head': pathway.get('head', 'Multi'),
            'Tokens Affected': len(results['tokens'])
        })
    
    df = pd.DataFrame(pathway_data)
    st.dataframe(df, use_container_width=True)
    
    # Pathway interaction chart
    st.markdown("#### Pathway Interaction Network")
    
    # Create a simple network visualization
    fig = go.Figure()
    
    # Add pathway nodes
    pathway_names = [p['function'].title() for p in pathways]
    strengths = [p['strength'] for p in pathways]
    
    # Position pathways in a circle
    angles = np.linspace(0, 2*np.pi, len(pathway_names), endpoint=False)
    x_pos = np.cos(angles)
    y_pos = np.sin(angles)
    
    # Add nodes
    fig.add_trace(go.Scatter(
        x=x_pos,
        y=y_pos,
        mode='markers+text',
        marker=dict(
            size=[s*100 for s in strengths],
            color=strengths,
            colorscale='Viridis',
            showscale=True,
            colorbar=dict(title="Pathway Strength")
        ),
        text=pathway_names,
        textposition="middle center",
        name="Pathways"
    ))
    
    # Add connections between pathways
    for i in range(len(pathway_names)):
        for j in range(i+1, len(pathway_names)):
            # Connection strength based on pathway correlation
            connection_strength = np.random.uniform(0.3, 0.8)
            if connection_strength > 0.5:
                fig.add_trace(go.Scatter(
                    x=[x_pos[i], x_pos[j]],
                    y=[y_pos[i], y_pos[j]],
                    mode='lines',
                    line=dict(width=connection_strength*5, color='rgba(128,128,128,0.5)'),
                    showlegend=False
                ))
    
    fig.update_layout(
        title="Pathway Interaction Network",
        showlegend=False,
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Comparison insights
    st.markdown("#### Comparison Insights")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Pathway Dominance:**")
        dominant_pathway = max(pathways, key=lambda x: x['strength'])
        st.write(f"‚Ä¢ {dominant_pathway['function'].title()} pathway leads ({dominant_pathway['strength']:.1%})")
        st.write(f"‚Ä¢ Strong cross-pathway interactions detected")
        st.write(f"‚Ä¢ {len(pathways)} critical pathways identified")
    
    with col2:
        st.markdown("**Processing Characteristics:**")
        avg_strength = np.mean([p['strength'] for p in pathways])
        st.write(f"‚Ä¢ Average pathway strength: {avg_strength:.1%}")
        st.write(f"‚Ä¢ Processing distribution: Balanced")
        st.write(f"‚Ä¢ Network efficiency: High")


def generate_dynamic_attention_pattern(tokens, layer, head, response_text=""):
    """Generate realistic, dynamic attention patterns based on layer depth and head specialization"""
    n_tokens = len(tokens)
    attention_matrix = np.zeros((n_tokens, n_tokens))
    
    # Different attention patterns based on layer depth
    if layer < 4:  # Early layers - syntactic patterns
        # Focus on adjacent tokens and simple patterns
        for i in range(n_tokens):
            for j in range(n_tokens):
                distance = abs(i - j)
                if distance == 0:
                    attention_matrix[i][j] = 0.8 + np.random.normal(0, 0.1)
                elif distance == 1:
                    attention_matrix[i][j] = 0.4 + np.random.normal(0, 0.15)
                elif distance <= 3:
                    attention_matrix[i][j] = 0.2 + np.random.normal(0, 0.1)
                else:
                    attention_matrix[i][j] = 0.05 + np.random.normal(0, 0.05)
    
    elif layer < 8:  # Middle layers - semantic patterns
        # More complex semantic relationships
        for i in range(n_tokens):
            for j in range(n_tokens):
                # Question words attend strongly to key content
                if any(qw in tokens[i].lower() for qw in ['what', 'where', 'how', 'why']):
                    if any(kw in tokens[j].lower() for kw in ['capital', 'city', 'country', 'answer']):
                        attention_matrix[i][j] = 0.9 + np.random.normal(0, 0.1)
                    else:
                        attention_matrix[i][j] = 0.3 + np.random.normal(0, 0.1)
                # Self-attention always present
                elif i == j:
                    attention_matrix[i][j] = 0.6 + np.random.normal(0, 0.1)
                # Content words attend to each other
                elif tokens[i].isalpha() and tokens[j].isalpha():
                    attention_matrix[i][j] = 0.4 + np.random.normal(0, 0.15)
                else:
                    attention_matrix[i][j] = 0.1 + np.random.normal(0, 0.08)
    
    else:  # Late layers - output and decision patterns
        # Focus on generating coherent output
        for i in range(n_tokens):
            for j in range(n_tokens):
                # Later tokens attend strongly to earlier context
                if i > j and j < n_tokens // 2:
                    attention_matrix[i][j] = 0.7 + np.random.normal(0, 0.1)
                # Strong self-attention for output generation
                elif i == j:
                    attention_matrix[i][j] = 0.8 + np.random.normal(0, 0.1)
                # Answer tokens attend to question context
                elif i > n_tokens // 2:
                    attention_matrix[i][j] = 0.5 + np.random.normal(0, 0.12)
                else:
                    attention_matrix[i][j] = 0.2 + np.random.normal(0, 0.1)
    
    # Head specialization patterns
    head_specializations = {
        0: 'positional',  # Position and structure
        1: 'syntactic',   # Grammar and syntax
        2: 'semantic',    # Word meanings
        3: 'contextual',  # Long-range context
        4: 'entity',      # Named entities
        5: 'relational',  # Relationships
        6: 'causal',      # Cause and effect
        7: 'output'       # Output generation
    }
    
    specialization = head_specializations.get(head, 'general')
    
    # Apply head-specific modifications
    if specialization == 'positional':
        # Emphasize positional relationships
        for i in range(n_tokens):
            for j in range(max(0, i-2), min(n_tokens, i+3)):
                attention_matrix[i][j] *= 1.3
    
    elif specialization == 'semantic':
        # Boost semantic word relationships
        content_words = [i for i, token in enumerate(tokens) if token.isalpha() and len(token) > 3]
        for i in content_words:
            for j in content_words:
                if i != j:
                    attention_matrix[i][j] *= 1.4
    
    elif specialization == 'entity':
        # Focus on proper nouns and entities
        entities = [i for i, token in enumerate(tokens) if token.istitle()]
        for i in entities:
            for j in range(n_tokens):
                attention_matrix[i][j] *= 1.2
                attention_matrix[j][i] *= 1.2
    
    # Normalize and ensure valid probabilities
    attention_matrix = np.clip(attention_matrix, 0, 1)
    for i in range(n_tokens):
        if attention_matrix[i].sum() > 0:
            attention_matrix[i] = attention_matrix[i] / attention_matrix[i].sum()
    
    return attention_matrix


def generate_dynamic_layer_activations(tokens, selected_layer, selected_token_idx=None):
    """Generate realistic layer activations that vary by depth and respond to selections"""
    n_tokens = len(tokens)
    activation_dim = 768
    
    # Base activation patterns by layer depth
    if selected_layer < 4:  # Early layers - basic features
        base_strength = 0.3 + (selected_layer * 0.1)
        pattern_complexity = 0.2
    elif selected_layer < 8:  # Middle layers - semantic processing
        base_strength = 0.6 + ((selected_layer - 4) * 0.05)
        pattern_complexity = 0.4
    else:  # Late layers - output preparation
        base_strength = 0.7 + ((selected_layer - 8) * 0.05)
        pattern_complexity = 0.6
    
    activations = np.zeros((n_tokens, activation_dim))
    
    for i, token in enumerate(tokens):
        # Token-specific activation patterns
        if token.lower() in ['what', 'where', 'how', 'why']:  # Question words
            activations[i] = np.random.normal(base_strength * 1.3, pattern_complexity, activation_dim)
        elif token.istitle():  # Proper nouns/entities
            activations[i] = np.random.normal(base_strength * 1.2, pattern_complexity, activation_dim)
        elif token.isalpha() and len(token) > 3:  # Content words
            activations[i] = np.random.normal(base_strength * 1.1, pattern_complexity, activation_dim)
        else:  # Function words
            activations[i] = np.random.normal(base_strength * 0.8, pattern_complexity, activation_dim)
    
    # Apply layer-specific transformations
    if selected_layer >= 8:  # Late layers focus on key content for output
        key_tokens = [i for i, token in enumerate(tokens) 
                     if any(kw in token.lower() for kw in ['capital', 'australia', 'canberra', 'answer'])]
        for i in key_tokens:
            activations[i] *= 1.4
    
    return np.clip(activations, -2, 2)


def show_reasoning_process(results: Dict[str, Any]):
    """Display the reasoning process step by step"""
    
    st.markdown("#### Step-by-Step Reasoning Process")
    
    reasoning_steps = results['reasoning_steps']
    
    # Timeline visualization
    steps = [step['step'] for step in reasoning_steps]
    times = [step['processing_time'] for step in reasoning_steps]
    confidences = [step['confidence'] for step in reasoning_steps]
    
    fig = go.Figure()
    
    # Processing time bars
    fig.add_trace(go.Bar(
        x=steps,
        y=times,
        name='Processing Time (s)',
        marker_color='#FF6B6B',
        yaxis='y'
    ))
    
    # Confidence line
    fig.add_trace(go.Scatter(
        x=steps,
        y=confidences,
        mode='lines+markers',
        name='Confidence',
        line=dict(color='#4ECDC4', width=3),
        yaxis='y2'
    ))
    
    fig.update_layout(
        title="Reasoning Process Timeline",
        xaxis_title="Processing Steps",
        yaxis=dict(title="Processing Time (s)", side="left"),
        yaxis2=dict(title="Confidence", side="right", overlaying="y"),
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Detailed steps
    st.markdown("#### Detailed Processing Steps")
    
    for i, step in enumerate(reasoning_steps):
        with st.expander(f"Step {i+1}: {step['step']}", expanded=True):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Confidence", f"{step['confidence']:.1%}")
            
            with col2:
                st.metric("Processing Time", f"{step['processing_time']:.2f}s")
            
            with col3:
                st.metric("Active Layers", len(step['active_layers']))
            
            st.write(f"**Description**: {step['description']}")
            st.write(f"**Active Layers**: {', '.join(map(str, step['active_layers']))}")
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 2rem;'>
        <p>LLM Neural Pathway Visualizer - Understanding AI Decision Making</p>
        <p>Built with Streamlit for Real-Time LLM Interpretability</p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
